{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Uncomment the following lines if necessary\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load documents\n",
    "collection = {}\n",
    "allfiles = []\n",
    "path = 'Corpus/'\n",
    "for ele in glob.glob(path + '*'):\n",
    "    filename = ele.split('/')[1]\n",
    "    allfiles.append(filename)\n",
    "    with open(ele) as f:\n",
    "        collection[filename] = f.read()\n",
    "\n",
    "allfiles = set(allfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def case_folding(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def normalization(tokens):\n",
    "    normalized_tokens = [re.sub(r'\\W+', '', token).strip() for token in tokens if token]\n",
    "    return normalized_tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def preprocess(text):\n",
    "    text = case_folding(text)\n",
    "    tokens = tokenization(text)\n",
    "    tokens = normalization(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatization(tokens)\n",
    "    return tokens\n",
    "\n",
    "dictionary = defaultdict(list)\n",
    "doc_lengths = defaultdict(float)\n",
    "doc_term_freq = defaultdict(lambda: defaultdict(int))\n",
    "total_docs = len(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dictionary\n",
    "for doc_id, content in collection.items():\n",
    "    tokens = preprocess(content)\n",
    "    term_freq = defaultdict(int)\n",
    "    \n",
    "    for token in tokens:\n",
    "        term_freq[token] += 1\n",
    "    \n",
    "    for term, freq in term_freq.items():\n",
    "        dictionary[term].append((doc_id, freq))\n",
    "        doc_term_freq[doc_id][term] = 1 + math.log10(freq)\n",
    "    \n",
    "    doc_length = 0\n",
    "    for freq in term_freq.values():\n",
    "        doc_length += (1 + math.log10(freq)) ** 2\n",
    "    doc_lengths[doc_id] = math.sqrt(doc_length)\n",
    "\n",
    "# Inverse Document Frequency calculation\n",
    "def calculate_idf(term):\n",
    "    if term in dictionary:\n",
    "        df = len(dictionary[term])\n",
    "        return math.log10(total_docs / df)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Cosine similarity calculation\n",
    "def cosine_similarity(query_vector, doc_vector, doc_id):\n",
    "    dot_product = sum(query_vector[term] * doc_vector.get(term, 0) for term in query_vector)\n",
    "    query_length = math.sqrt(sum(weight ** 2 for weight in query_vector.values()))\n",
    "    doc_length = doc_lengths.get(doc_id, 0)\n",
    "    if query_length * doc_length == 0:\n",
    "        return 0\n",
    "    return dot_product / (query_length * doc_length)\n",
    "\n",
    "def process_query(query):\n",
    "    tokens = preprocess(query)\n",
    "    query_term_freq = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        query_term_freq[token] += 1\n",
    "    return query_term_freq\n",
    "\n",
    "# Ranked retrieval\n",
    "def ranked_retrieval(query):\n",
    "    query_term_freq = process_query(query)\n",
    "    query_vector = {}\n",
    "    \n",
    "    # tf-idf\n",
    "    for term, freq in query_term_freq.items():\n",
    "        idf = calculate_idf(term)\n",
    "        query_vector[term] = (1 + math.log10(freq)) * idf\n",
    "    \n",
    "    # cosine similarity\n",
    "    scores = []\n",
    "    for doc_id in collection:\n",
    "        doc_vector = doc_term_freq[doc_id]\n",
    "        score = cosine_similarity(query_vector, doc_vector, doc_id)\n",
    "        if score > 0:\n",
    "            scores.append((doc_id, score))\n",
    "    \n",
    "    scores = sorted(scores, key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    return scores[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 output:\n",
      "('zomato.txt', 0.2057964082143958)\n",
      "('swiggy.txt', 0.11943668052188824)\n",
      "('instagram.txt', 0.0562340462953861)\n",
      "('messenger.txt', 0.055177109640057447)\n",
      "('youtube.txt', 0.04539769786533345)\n",
      "('reddit.txt', 0.044212755005046386)\n",
      "('bing.txt', 0.0417471942151515)\n",
      "('flipkart.txt', 0.03938035947139302)\n",
      "('HP.txt', 0.038484634396480664)\n",
      "('paypal.txt', 0.03841568305484799)\n",
      "\n",
      "Query 2 output:\n",
      "('shakespeare.txt', 0.11891957091796325)\n",
      "('levis.txt', 0.023773719199982342)\n",
      "('nike.txt', 0.018298548721953117)\n",
      "('Adobe.txt', 0.015604571767964412)\n",
      "('zomato.txt', 0.014989515510215642)\n",
      "('huawei.txt', 0.0134930594236145)\n",
      "('skype.txt', 0.011974161946371902)\n",
      "('blackberry.txt', 0.011331049715597837)\n",
      "('reliance.txt', 0.010428090643860338)\n",
      "('Dell.txt', 0.010339228996537778)\n"
     ]
    }
   ],
   "source": [
    "# Sample test cases\n",
    "query1 = \"Developing your Zomato business account and profile is a great way to boost your restaurant's online reputation\"\n",
    "query2 = \"Warwickshire, came from an ancient family and was the heiress to some land\"\n",
    "\n",
    "result1 = ranked_retrieval(query1)\n",
    "result2 = ranked_retrieval(query2)\n",
    "\n",
    "def print_results(result):\n",
    "    for i in result:\n",
    "        print(i)\n",
    "\n",
    "print(\"Query 1 output:\")\n",
    "print_results(result1)\n",
    "print()\n",
    "print(\"Query 2 output:\")\n",
    "print_results(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
